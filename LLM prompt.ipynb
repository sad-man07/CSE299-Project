{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKqGn0mplSHt3Nbh81HK5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sad-man07/CSE299-Project/blob/main/LLM%20prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cVs8ERgaPGM",
        "outputId": "f135195c-2f75-4540-800c-fd5a62730900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.38.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n"
      ],
      "metadata": {
        "id": "tJ9XDofbaWw9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "metadata": {
        "id": "bvndX25nuv2b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")"
      ],
      "metadata": {
        "id": "8EWYYjTXuyGn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")"
      ],
      "metadata": {
        "id": "_9vk30Cwah2Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "DcwkSibqfHkh",
        "outputId": "6b009a79-8c0f-4f30-d4e4-f1e7f6834f4a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.bloom.modeling_bloom.BloomForCausalLM"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.bloom.modeling_bloom.BloomForCausalLM</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py</a>The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
              "embeddings).\n",
              "\n",
              "\n",
              "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
              "library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)\n",
              "\n",
              "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
              "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
              "and behavior.\n",
              "\n",
              "Parameters:\n",
              "    config ([`BloomConfig`]): Model configuration class with all the parameters of the model.\n",
              "        Initializing with a config file does not load the weights associated with the model, only the\n",
              "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 756);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__.__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CKd-0Eg2jWs9",
        "outputId": "2016985c-138f-4142-900a-16129f8da7c7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BloomForCausalLM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(11111)"
      ],
      "metadata": {
        "id": "fYv4KfImjrJn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt = \"Who is George Washington?\""
      ],
      "metadata": {
        "id": "OuRw96-vkb26"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = tokenizer(text_prompt, return_tensors=\"pt\").to(0)"
      ],
      "metadata": {
        "id": "ED-sneMHlFbt"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens"
      ],
      "metadata": {
        "id": "ybXFJgoPlQAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a21b1c-9c1f-49bb-9f0b-5427efb16642"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[57647,   632, 20773, 20006,    34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_sample = model.generate(**input_tokens, max_length=200, top_k=0, temperature=0.5)"
      ],
      "metadata": {
        "id": "hkTHWZwIlwIl"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vguTavWrqatL",
        "outputId": "573d9309-e3b4-4c8a-90df-66e241836d0a"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[57647,   632, 20773, 20006,    34,   982,   603,  1502,  5563,  1620,\n",
              "           368,  3968, 14032,   461,   368,  8097, 10650,    15,   982,  7555,\n",
              "           368, 21380,  6149,  1502,  5024,  1683,  1620,   267, 10087,  1517,\n",
              "            15,   982,  7555,   368, 52242,  6149,  1502, 31559,    15,   982,\n",
              "          7555,   368, 21380,    15,  1503,   392,  1683,  1620,   267, 10087,\n",
              "          1517,    17,   982,   603,  1502,  5024,  1683,  1620,   267, 10087,\n",
              "          1517,    15,   982,  7555,   368, 52242,  6149,  1502,  5024,  1683,\n",
              "          1620,   267, 10087,  1517,    15,   982,  7555,   368, 21380,  6149,\n",
              "          1502,  5024,  1683,  1620,   267, 10087,  1517,    15,   982,  7555,\n",
              "           368, 52242,  6149,  1502,  5024,  1683,  1620,   267, 10087,  1517,\n",
              "            15,   982,  7555,   368, 21380,  6149,  1502,  5024,  1683,  1620,\n",
              "           267, 10087,  1517,    15,   982,  7555,   368, 52242,  6149,  1502,\n",
              "          5024,  1683,  1620,   267, 10087,  1517,    15,   982,  7555,   368,\n",
              "         21380,  6149,  1502,  5024,  1683,  1620,   267, 10087,  1517,    15,\n",
              "           982,  7555,   368, 52242,  6149,  1502,  5024,  1683,  1620,   267,\n",
              "         10087,  1517,    15,   982,  7555,   368, 21380,  6149,  1502,  5024,\n",
              "          1683,  1620,   267, 10087,  1517,    15,   982,  7555,   368, 52242,\n",
              "          6149,  1502,  5024,  1683,  1620,   267, 10087,  1517,    15,   982,\n",
              "          7555,   368, 21380,  6149,  1502,  5024,  1683,  1620,   267, 10087,\n",
              "          1517,    15,   982,  7555,   368, 52242,  6149,  1502,  5024,  1683]])"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(result_sample[0], truncate_before_pattern=[r\"\\n\\n^#\", r\"^'''\", r\"\\n\\n\\n\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFJE8kTjvNuM",
        "outputId": "4ca32a06-5a2a-462c-ab58-c31f530eb416"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is George Washington?”\n",
            "\n",
            "“He was the first President of the United States,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“Yes,” said the boy, “and he was a great man.”\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he was a great man,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he was a great man,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he was a great man,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he was a great man,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he was a great man,” said the boy.\n",
            "\n",
            "“And he was a great man,” said the teacher.\n",
            "\n",
            "“And he\n"
          ]
        }
      ]
    }
  ]
}